Title : Notes on The Elements of Statistical Learning

pre,code {
  language: python;
}

equation {
  toc: equations;
  toc-line: "&label;&ensp;&caption;";
}

Doc class : [onecolumn, a4paper, 12pt]article

[INCLUDE=book]
[INCLUDE=webtoc]
[INCLUDE=webanchors]


[TITLE]


~ Begin SidePanel
[TOC]
~ End SidePanel


~ Begin MainPanel

# Chapter 2 Overview of Supervised Learning

## Linear Models
Given a vector of inputs $X^T = (X_1,X_2,... ,X_p)$, we predict the output $Y$ via the model 

~ Equation
\hat{Y} = \hat{\beta_0} + \sum_{j=1}^p X_j \hat{\beta_j}
~

The term $\hat{\beta_0}$ is the intercept(截距), also known as the bias(偏差) in machine learning. Often it is convenient to include the constant variable $1$ in $X$, include $\hat{\beta_0}$ in the vector of coefficients $\hat{\beta}$, and then write the linear model in vector form as an inner product  $\hat{Y} = X^T \hat{\beta}$.

Viewed as a function over the p-dimensional input space, $f(X) = X^T\beta$ is linear, and the gradient $f′(X) = \beta$ is a vector in input space that points in the steepest uphill direction.

The most popular is the method to fit the linear model to a set of training data is least squares. In this approach, we pick the coefficients $\beta$ to minimize the residual sum of squares

~ Equation
\begin{split}
RSS(\beta) &= \sum_{i=1}^N(y_i − x^T_i \beta)^2 \\
           &= (\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)
\end{split}
~

~ Note
两个元素乘积的和常常转化成向量内积或矩阵相乘来处理，即：
~
~ Equation
\sum_{i=1}^N \alpha_i\beta_i = (\alpha_1, \alpha_2, \cdots, \alpha_n)^T (\beta_1, \beta_2, \cdots, \beta_n)
~

~ End MainPanel