Title : Installation

equation {
  toc: equations;
  toc-line: "&label;&ensp;&caption;";
}

.pre-fenced3 {
  language: python;
}

.code1 {
  color: Red;
}

Doc class : [onecolumn, a4paper, 12pt]article

Package: [curve]xypic
Package: amscd
Package: amsfonts
Package: amsmath
Package: fontspec
Package: xeCJK
Package: geometry
Package: pgfplots
Package: pgfplotstable
Package: pst-plot
Package: pstricks
Package: tikz

Tex Header:
  \setmainfont{Times New Roman}
  \setsansfont{Arial}
  \setmonofont{Courier New}
  \setCJKmainfont[BoldFont={SimHei},ItalicFont={SimHei}]{SimSun}
  \setCJKsansfont[BoldFont={SimHei},ItalicFont={SimHei}]{SimSun}
  \setCJKmonofont[BoldFont={SimHei},ItalicFont={SimHei}]{SimSun}
  \XeTeXlinebreaklocale "zh"
  \XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt
  \geometry{top=1.2in,bottom=1.2in,left=1.2in,right=1in}
  \usetikzlibrary{decorations.pathreplacing%
  ,decorations.pathmorphing}

[INCLUDE=book]
[INCLUDE=webtoc]
[INCLUDE=webanchors]

[TITLE]


~ Begin SidePanel
[TOC]
~ End SidePanel


~ Begin MainPanel

# Quick Start

## Define A DAG Pipeline Script
Airflow Python script is really just a configuration file specifying the DAG’s structure as code. The actual tasks defined here will run in a different context from the context of this script. Different tasks run on different workers at different points in time, which means that this script cannot be used to cross communicate between tasks. Note that for this purpose we have a more advanced feature called `XCom`.

### Default Arguments
Before creating a DAG and some tasks, we can pass a set of arguments to each task’s constructor which would become redundant, or we can define a dictionary of default parameters that we can use when creating tasks.

```{language=python background-color=Gainsboro padding=2ex }
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2015, 6, 1),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
}
```

### Instantiate a DAG Object
We’ll need a DAG object to nest our tasks into.

```{ language=python background-color=Gainsboro padding=2ex }
dag = DAG(
    'tutorial', default_args=default_args, schedule_interval=timedelta(1))
```

### Tasks
Tasks are generated when instantiating operator objects.

```{ language=python background-color=Gainsboro padding=2ex }
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag)

t2 = BashOperator(
    task_id='sleep',
    bash_command='sleep 5',
    retries=3,
    dag=dag)
```

A task must include or inherit the arguments `task_id` and `owner`, otherwise Airflow will raise an exception. The precedence rules for a task are as follows:

1. Explicitly passed arguments
2. Values that exist in the default_args dictionary
3. The operator’s default value, if one exists

### Templating with Jinja
Airflow leverages the power of [Jinja Templating](http://jinja.pocoo.org/docs/dev/) and provides the pipeline author with a set of built-in parameters and macros. Airflow also provides hooks for the pipeline author to define their own parameters, macros and templates.

```{ language=python background-color=Gainsboro padding=2ex }
templated_command = """
    {% for i in range(5) %}
        echo "{{ ds }}"
        echo "{{ macros.ds_add(ds, 7) }}"
        echo "{{ params.my_param }}"
    {% endfor %}
"""

t3 = BashOperator(
    task_id='templated',
    bash_command=templated_command,
    params={'my_param': 'Parameter I passed in'},
    dag=dag)
```

Files can also be passed to the `bash_command` argument, like `bash_command='templated_command.sh'`, where the file location is relative to the directory containing the pipeline file. This may be desirable for many reasons, like separating your script’s logic and pipeline code, allowing for proper code highlighting in files composed in different languages, and general flexibility in structuring pipelines. It is also possible to define your `template_searchpath` as pointing to any folder locations in the DAG constructor call.

- The `templated_command` contains code logic in `{% %}` blocks, references parameters like `{{ ds }}`, calls a function as in `{{ macros.ds_add(ds, 7)}}`, and references a user-defined parameter in `{{ params.my_param }}`.

- The `params` hook in `BaseOperator` allows you to pass a dictionary of parameters and/or objects to your templates.

- The `user_defined_macros` allows you to specify your own variables. For example, passing `dict(foo='bar')` to this argument allows you to use `{{ foo }}` in your templates.

- The `user_defined_filters` allow you to register you own filters. For example, passing `dict(hello=lambda name: 'Hello %s' % name)` to this argument allows you to use `{{ 'world' | hello }}` in your templates. For more information regarding custom filters have a look at the [Jinja Documentation](http://jinja.pocoo.org/docs/dev/api/#writing-filters).

### Setting up Dependencies

We have two simple tasks that do not depend on each other. Here’s a few ways you can define dependencies between them:

```{ language=python background-color=Gainsboro padding=2ex }
t2.set_upstream(t1)

# This means that t2 will depend on t1 
# running successfully to run
# It is equivalent to 
# t1.set_downstream(t2)

t3.set_upstream(t1)

# all of this is equivalent to
# dag.set_dependency('print_date', 'sleep')
# dag.set_dependency('print_date', 'templated')
```

Note that when executing your script, Airflow will raise exceptions when it finds cycles in your DAG or when a dependency is referenced more than once.

### Recap
Alright, so we have a pretty basic DAG. At this point your code should look something like this:

```{ language=python background-color=Gainsboro padding=2ex }
"""
Code that goes along with the Airflow located at:
http://airflow.readthedocs.org/en/latest/tutorial.html
"""
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2015, 6, 1),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
}

dag = DAG(
    'tutorial', default_args=default_args, schedule_interval=timedelta(1))

# t1, t2 and t3 are examples of tasks created by instantiating operators
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag)

t2 = BashOperator(
    task_id='sleep',
    bash_command='sleep 5',
    retries=3,
    dag=dag)

templated_command = """
    {% for i in range(5) %}
        echo "{{ ds }}"
        echo "{{ macros.ds_add(ds, 7)}}"
        echo "{{ params.my_param }}"
    {% endfor %}
"""

t3 = BashOperator(
    task_id='templated',
    bash_command=templated_command,
    params={'my_param': 'Parameter I passed in'},
    dag=dag)

t2.set_upstream(t1)
t3.set_upstream(t1)
```

## Testing

### Running the Script
Time to run some tests. First let’s make sure that the pipeline parses. Let’s assume we’re saving the code from the previous step in tutorial.py in the DAGs folder referenced in your airflow.cfg. The default location for your DAGs is ~/airflow/dags.

```{ background-color=Gainsboro padding=2ex }
python ~/airflow/dags/tutorial.py
```

If the script does not raise an exception it means that you haven’t done anything horribly wrong, and that your Airflow environment is somewhat sound.

### Command Line Metadata Validation
Let’s run a few commands to validate this script further.

```{ background-color=Gainsboro padding=2ex }
# print the list of active DAGs
airflow list_dags

# prints the list of tasks the "tutorial" dag_id
airflow list_tasks tutorial

# prints the hierarchy of tasks in the tutorial DAG
airflow list_tasks tutorial --tree
```

### Testing
Let’s test by running the actual task instances on a specific date. The date specified in this context is an `execution_date`, which simulates the scheduler running your task or dag at a specific date + time:

```{ background-color=Gainsboro padding=2ex }
# command layout: command subcommand dag_id task_id date

# testing print_date
airflow test tutorial print_date 2015-06-01

# testing sleep
airflow test tutorial sleep 2015-06-01

# testing templated
airflow test tutorial templated 2015-06-01
```

Note that the airflow test command runs task instances locally, outputs their log to stdout on screen, doesn’t bother with dependencies, and doesn’t communicate state to the database. It simply allows testing a single task instance.

### Backfill
Everything looks like it’s running fine so let’s run a `backfill`. backfill will respect your dependencies, emit logs into files and talk to the database to record status. If you do have a webserver up, you’ll be able to track the progress. airflow webserver will start a web server if you are interested in tracking the progress visually as your backfill progresses.

Note that if you use `depends_on_past=True`, individual task instances will depend on the success of the preceding task instance, except for the `start_date` specified itself, for which this dependency is disregarded.

The date range in this context is a `start_date` and optionally an `end_date`, which are used to populate the run schedule with task instances from this dag.

```{ background-color=Gainsboro padding=2ex }
# optional, start a web server in debug mode in the background
# airflow webserver --debug &

# start your backfill on a date range
airflow backfill tutorial -s 2015-06-01 -e 2015-06-07
```

~ End MainPanel